{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-23T07:40:50.808293Z",
     "start_time": "2024-01-23T07:40:50.395429Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn.experimental import enable_halving_search_cv # explicitly require this experimental feature\n",
    "from sklearn.model_selection import KFold, HalvingGridSearchCV, train_test_split, ShuffleSplit\n",
    "from sklearn.svm import LinearSVC, SVC"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading Features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "617743bdda2fa6fd"
  },
  {
   "cell_type": "code",
   "source": [
    "# Read EEG Features\n",
    "features = pd.read_csv('all_features.csv', index_col=0)\n",
    "features = features.dropna(subset=['targets'])\n",
    "scores_max, scores_min = max(features['targets']), min(features['targets'])\n",
    "N_CLASSES = 2\n",
    "for subject_session in features.index:\n",
    "    score = features.loc[subject_session]['targets']\n",
    "    score = int((score-1e-6 - scores_min) / (scores_max - scores_min) * N_CLASSES)  # discretise\n",
    "    features.loc[subject_session, 'targets'] = score\n",
    "features.loc[:, 'targets'] = features['targets'].astype(int)\n",
    "\n",
    "# Normalize each feature column\n",
    "for column in features.columns[:-1]:\n",
    "    features.loc[:, column] = (features[column] - features[column].min()) / (features[column].max() - features[column].min())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T07:40:52.526896Z",
     "start_time": "2024-01-23T07:40:50.810366Z"
    }
   },
   "id": "4271b4bdca81498a",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove columns completely filled with NaNs\n",
    "features = features.dropna(axis=1, how='all')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T07:40:52.541768Z",
     "start_time": "2024-01-23T07:40:52.536208Z"
    }
   },
   "id": "c52b60dd70219de5",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Keep only some features?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc280f99481b279"
  },
  {
   "cell_type": "markdown",
   "source": [
    "From file? "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7be583da26bde96"
  },
  {
   "cell_type": "code",
   "source": [
    "#selected_features = np.loadtxt('results/optimal_pli_only.txt', dtype=str)\n",
    "selected_features = np.concatenate((np.loadtxt('results/optimal_pli_only.txt', dtype=str), np.loadtxt('results/optimal_hjorth_only.txt', dtype=str)))\n",
    "selected_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T07:40:52.601084Z",
     "start_time": "2024-01-23T07:40:52.544098Z"
    }
   },
   "id": "301cb1b884216a20",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select immediately? (e.g. to make feature selection on this subset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f65c26a5c7e9f2f7"
  },
  {
   "cell_type": "code",
   "source": [
    "features = features[np.append(selected_features, ['targets', ])]  # keep only the selected features and the targets\n",
    "features.dropna(axis=0, inplace=True)  # Discard subjects with NaNs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T07:40:52.601665Z",
     "start_time": "2024-01-23T07:40:52.564672Z"
    }
   },
   "id": "9a1951a85b474876",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just some groups?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57b088e27e178c83"
  },
  {
   "cell_type": "code",
   "source": [
    "#targets = features.loc[:, 'targets']\n",
    "\n",
    "# Only Spectral\n",
    "#features = features.iloc[:, :600]\n",
    "\n",
    "# Only Hjorth\n",
    "#features = features.iloc[:, 600:666]\n",
    "\n",
    "# Only PLI\n",
    "#features = features.iloc[:, 666:-1]\n",
    "\n",
    "\n",
    "###########################\n",
    "#features.loc[:,'targets'] = targets  # restore targets\n",
    "#features = features.dropna()  # remove NaNs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T07:40:52.601953Z",
     "start_time": "2024-01-23T07:40:52.573520Z"
    }
   },
   "id": "fadf9195b503ae90",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f236215bd44cc7c9"
  },
  {
   "cell_type": "code",
   "source": [
    "features.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T07:40:52.691972Z",
     "start_time": "2024-01-23T07:40:52.592609Z"
    }
   },
   "id": "aa55d5982c0e0eeb",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ee45457487c01e3"
  },
  {
   "cell_type": "code",
   "source": [
    "params = {'class_weight': None,\n",
    "          'criterion': 'gini',\n",
    "          'max_depth': 5,\n",
    "          'max_features': 'sqrt',\n",
    "          'max_samples': None,\n",
    "          'min_samples_leaf': 0.51,\n",
    "          'min_samples_split': 5,\n",
    "          'n_estimators': 500,\n",
    "          'random_state': 0}\n",
    "#model = RandomForestClassifier(**params)\n",
    "model = RandomForestClassifier(500, max_depth=5, random_state=0) #, oob_score=True, warm_start=True)\n",
    "#model = GradientBoostingClassifier(loss='exponential', n_estimators=100, criterion='friedman_mse', max_depth=3, learning_rate=0.1, random_state=0)\n",
    "#model = SVC(kernel='rbf', C=4, class_weight='balanced', random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T07:40:52.693045Z",
     "start_time": "2024-01-23T07:40:52.624281Z"
    }
   },
   "id": "1c40c0d6f53525b8",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run Purpose"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65a6328a55870f0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cross-Validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "131254d0a683cebb"
  },
  {
   "cell_type": "code",
   "source": [
    "CV = True\n",
    "# 10-fold cross-validation\n",
    "#cv = StratifiedShuffleSplit(n_splits=10, random_state=0)\n",
    "#cv = StratifiedKFold(n_splits=10)  # sem shuffle\n",
    "#cv = ShuffleSplit(n_splits=10, random_state=0)  # sem estratificação\n",
    "#cv = KFold(n_splits=5)  # sem shuffle nem estratificação\n",
    "# Leave-4-out cross-validation\n",
    "#cv = StratifiedShuffleSplit(n_splits=179-4, random_state=0)\n",
    "#cv = ShuffleSplit(n_splits=179-4, random_state=0)  # sem estratificação\n",
    "#cv = KFold(n_splits=176-4)#len(features)-4)  # sem shuffle nem estratificação\n",
    "cv = KFold(n_splits=len(features) // 4)  # sem shuffle nem estratificação\n",
    "# My cross-validation\n",
    "#cv = StratifiedLeavePOut(C=2, P=4, with_repetition=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T07:40:52.693807Z",
     "start_time": "2024-01-23T07:40:52.634945Z"
    }
   },
   "id": "f843310825fa1dc7",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Sort subjects?\n",
    "features = features.sort_index()\n",
    "# Remove subjects without session pair?\n",
    "features = features.drop(labels=['005_2', '102_2', '109_1', '195_2', '303_2'])\n",
    "features.dropna(axis=0, inplace=True)  # Discard subjects with NaNs\n",
    "features.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T07:40:52.726332Z",
     "start_time": "2024-01-23T07:40:52.643363Z"
    }
   },
   "id": "af9559f7393ea6e7",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d4f2829a6c51058"
  },
  {
   "cell_type": "code",
   "source": [
    "SELECTION = True\n",
    "selector = RFECV\n",
    "min_features_to_select = 10\n",
    "max_features_to_select = 24\n",
    "n_features_to_select = 24"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T07:40:52.726913Z",
     "start_time": "2024-01-23T07:40:52.698545Z"
    }
   },
   "id": "1534a5a29337eec1",
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameters Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54b701fad5b4dda7"
  },
  {
   "cell_type": "code",
   "source": [
    "HYPERPARAMETERS_TUNING = False\n",
    "tunner = HalvingGridSearchCV\n",
    "hyperparameters = (\n",
    "    {'n_estimators': [100, 500, 1000],\n",
    "     'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "     'max_depth': [5, 10, 20, None],\n",
    "     'min_samples_split': [5, 10, 0.51],\n",
    "     'min_samples_leaf': [1, 2, 0.51],\n",
    "     'max_features': ['sqrt', 'log2'],\n",
    "     'class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "     'max_samples': [None, 0.5, 0.75, 0.9],\n",
    "     }\n",
    ")  # n_candidates: 7776"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T07:40:52.826763Z",
     "start_time": "2024-01-23T07:40:52.711283Z"
    }
   },
   "id": "13a9e9aebe88da2b",
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sequential Selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91c5eef7476233e8"
  },
  {
   "cell_type": "code",
   "source": [
    "SEQUENTIAL_SELECTION = False\n",
    "additional_features = None  # pli_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T07:40:52.827501Z",
     "start_time": "2024-01-23T07:40:52.724602Z"
    }
   },
   "id": "8a66cf7f9aca82c2",
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a1ee778ba55e990"
  },
  {
   "cell_type": "code",
   "source": [
    "if SELECTION and CV and not HYPERPARAMETERS_TUNING:\n",
    "    selector_cv = selector(estimator=model, step=1, cv=cv,\n",
    "                            scoring='f1_weighted', min_features_to_select=min_features_to_select,\n",
    "                            verbose=2, n_jobs=-1)\n",
    "\n",
    "    selector_cv.fit(features.iloc[:, :-1], features['targets'])\n",
    "    print(f\"Optimal number of features: {selector_cv.n_features_}\")\n",
    "\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    n_scores = len(selector_cv.cv_results_[\"mean_test_score\"])\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Mean test accuracy\")\n",
    "    plt.scatter(\n",
    "        range(min_features_to_select, n_scores + min_features_to_select),\n",
    "        selector_cv.cv_results_[\"mean_test_score\"],\n",
    "        #yerr=selector_cv.cv_results_[\"std_test_score\"],\n",
    "    )\n",
    "    plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "    plt.show()\n",
    "\n",
    "    # Get optimal features\n",
    "    selected_features = selector_cv.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T08:00:31.700431Z",
     "start_time": "2024-01-23T07:40:52.743091Z"
    }
   },
   "id": "93cc41fdab3f3e58",
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if SELECTION and not CV and not HYPERPARAMETERS_TUNING:\n",
    "    #selector = RFE(estimator=model, n_features_to_select=100, step=1, verbose=2)\n",
    "    #selector = selector(estimator=model, max_features=max_features_to_select)\n",
    "    selector = selector(LinearSVC(dual=True, penalty=\"l2\", loss='hinge'), max_features=max_features_to_select)\n",
    "    selector.fit(features.iloc[:, :-1], features['targets'])\n",
    "    selected_features = selector.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T08:00:31.705137Z",
     "start_time": "2024-01-23T08:00:31.701188Z"
    }
   },
   "id": "9dac8c05eda09054",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "selected_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T08:00:31.714026Z",
     "start_time": "2024-01-23T08:00:31.707421Z"
    }
   },
   "id": "34b3aba9ebb9385",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "features = features[np.append(selected_features, ['targets', ])]  # keep only the selected features and the targets\n",
    "features.dropna(axis=0, inplace=True)  # Discard subjects with NaNs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T08:00:31.751332Z",
     "start_time": "2024-01-23T08:00:31.719892Z"
    }
   },
   "id": "5eb602bd16d7e4ef",
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if CV and not SELECTION and not HYPERPARAMETERS_TUNING:\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    #print(\"Cross-Validation with StratifiedLeavePOut\")\n",
    "    #print(\"Number of splits:\", cv.get_n_splits(features['targets']))\n",
    "    #print(\"Size of test sets\", cv.p * cv.c * 2)\n",
    "    scores = cross_val_score(model, features[selected_features], features['targets'],\n",
    "                              cv=cv, scoring='f1_weighted', verbose=2, n_jobs=-1)\n",
    "    print(\"Cross-Validation mean score:\", scores.mean())\n",
    "    print(\"Cross-Validation std score:\", scores.std())\n",
    "    print(\"Cross-Validation max score:\", scores.max())\n",
    "    print(\"Cross-Validation min score:\", scores.min())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T08:00:31.752067Z",
     "start_time": "2024-01-23T08:00:31.729654Z"
    }
   },
   "id": "866d736a7091411c",
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if not CV and not SELECTION and not HYPERPARAMETERS_TUNING and not SEQUENTIAL_SELECTION:\n",
    "    # Split subjects into train and test (using sklearn)\n",
    "    train_size = 0.98\n",
    "    n_train = int(len(features) * train_size)\n",
    "    n_test = len(features) - n_train\n",
    "    train_dataset, test_dataset = train_test_split(features, train_size=train_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   #stratify=features['targets'],\n",
    "                                                   random_state=1)\n",
    "    train_features = train_dataset[selected_features]\n",
    "    train_targets = train_dataset['targets']\n",
    "    test_features = test_dataset[selected_features]\n",
    "    test_targets = test_dataset['targets']\n",
    "    print(\"Train features shape:\", train_features.shape)\n",
    "    print(\"Test features shape:\", test_features.shape)\n",
    "\n",
    "    # Train\n",
    "    model = model.fit(train_features, train_targets)\n",
    "\n",
    "    #test_features = train_features  # FIXME: remove this line\n",
    "    #test_targets = train_targets  # FIXME: remove this line\n",
    "\n",
    "    # Test\n",
    "    predictions = model.predict(test_features)\n",
    "    # Adjust predictions to the number of classes\n",
    "    #predictions_min, predictions_max = 0, 1\n",
    "    #for i in range(len(predictions)):\n",
    "    #    predictions[i] = int((predictions[i]-1e-6 - predictions_min) / (predictions_max - predictions_min) * N_CLASSES)  # discretise\n",
    "\n",
    "    # 7.1) Accuracy\n",
    "    accuracy = accuracy_score(test_targets, predictions)\n",
    "\n",
    "    # 7.2) F1-Score\n",
    "    f1 = f1_score(test_targets, predictions, average='weighted')\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'F1-Score: {f1}')\n",
    "    print('-----\\n')\n",
    "\n",
    "    # 8. Plot Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "    import seaborn as sn\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    cm = confusion_matrix(test_targets, predictions)\n",
    "    df_cm = pd.DataFrame(cm, range(N_CLASSES), range(N_CLASSES))\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sn.set(font_scale=1.4)  # for label size\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})  # font size\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T08:00:31.752479Z",
     "start_time": "2024-01-23T08:00:31.744884Z"
    }
   },
   "id": "14b58b91a6f54d01",
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if HYPERPARAMETERS_TUNING and CV and not SELECTION and not SEQUENTIAL_SELECTION:\n",
    "    print(\"Hyperparameter tuning\")\n",
    "    #_tunner = tunner(model, hyperparameters, resource='n_samples', min_resources=int(179*0.5),\n",
    "    #                 cv=cv, scoring='f1_weighted', error_score=0, random_state=0,\n",
    "    #                 verbose=2, n_jobs=-1)\n",
    "\n",
    "    _tunner = tunner(model, hyperparameters,\n",
    "                     cv=cv, scoring='f1_weighted', error_score=0, random_state=0,\n",
    "                     verbose=2, n_jobs=-1)\n",
    "\n",
    "    _tunner.fit(features[selected_features], features['targets'])\n",
    "    print(\"Finished hyperparameter tuning\")\n",
    "    print(\"Best parameters:\", _tunner.best_params_)\n",
    "    print(\"Best score:\", _tunner.best_score_)\n",
    "    print(\"All results:\")\n",
    "    all_results = pd.DataFrame(_tunner.cv_results_)\n",
    "    print(all_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T08:00:31.786657Z",
     "start_time": "2024-01-23T08:00:31.754692Z"
    }
   },
   "id": "ccb33e86550d5b47",
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if SEQUENTIAL_SELECTION and not SELECTION and not CV and not HYPERPARAMETERS_TUNING:\n",
    "\n",
    "    def _train_test(model, experimental_features, train_size=0.85):\n",
    "        train_dataset, test_dataset = train_test_split(features, train_size=train_size, shuffle=True, stratify=features['targets'], random_state=0)\n",
    "        train_features, train_targets = train_dataset[experimental_features], train_dataset['targets']\n",
    "        test_features, test_targets = test_dataset[experimental_features], test_dataset['targets']\n",
    "        model.fit(train_features, train_targets)\n",
    "        predictions = model.predict(test_features)\n",
    "        f1 = f1_score(test_targets, predictions, average='weighted')\n",
    "        return f1\n",
    "\n",
    "    # Baseline test\n",
    "    f1_baseline = _train_test(RandomForestClassifier(**params), selected_features)\n",
    "\n",
    "    # Iterate through 'additional_features' and add the one that improves the most the model\n",
    "    print(\"Sequential feature selection\")\n",
    "    results = {'baseline': f1_baseline}\n",
    "    for feature in additional_features.columns:\n",
    "        experimental_set_features = np.append(selected_features, feature)\n",
    "        f1 = _train_test(RandomForestClassifier(**params), experimental_set_features, train_size=0.98)\n",
    "        print(f\"With '{feature}'\\nF1-score = {f1}\\n\")\n",
    "        results[feature] = f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T08:00:31.787061Z",
     "start_time": "2024-01-23T08:00:31.767192Z"
    }
   },
   "id": "a2a7c6e5ff154c6f",
   "execution_count": 21,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
